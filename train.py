import os
import requests
import zipfile
import torch
import torch.nn as nn
import shutil
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from transformers import DetrImageProcessor, DetrForObjectDetection
from PIL import Image
from sklearn.metrics import confusion_matrix
from torch.cuda.amp import autocast, GradScaler # [ì¶”ê°€] í˜¼í•© ì •ë°€ë„ìš©

# =========================================================
# âš™ï¸ ì„¤ì • (Hyperparameters) - ë°°ì¹˜ 32 íš¨ê³¼ ë‚´ê¸°!
# =========================================================
EPOCHS = 300            # ì¶”ê°€ í•™ìŠµ íšŸìˆ˜
LOAD_DIR = "./detr-visdrone-best" # ì´ì–´ì„œ í•™ìŠµí•  ëª¨ë¸ ê²½ë¡œ
SAVE_DIR = "./detr-visdrone-final" # ìµœì¢… ì €ì¥ ê²½ë¡œ

# [í•µì‹¬ ì„¤ì •]
PHYSICAL_BATCH_SIZE = 8   # GPUì— ì‹¤ì œë¡œ ë“¤ì–´ê°€ëŠ” ì–‘ (4080 ì•ˆì „ë¹µ)
TARGET_BATCH_SIZE = 32    # ìš°ë¦¬ê°€ ì›í•˜ëŠ” í•™ìŠµ íš¨ê³¼ (ë°°ì¹˜ 32)
ACCUMULATION_STEPS = TARGET_BATCH_SIZE // PHYSICAL_BATCH_SIZE # 32 / 8 = 4ë²ˆ ëª¨ì•„ì„œ ì¨

NUM_WORKERS = 4         
LEARNING_RATE = 1e-6    # ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ ë‚®ì¶¤ (ì´ë¯¸ ë˜‘ë˜‘í•´ì¡Œìœ¼ë‹ˆê¹Œ)
DATA_DIR = "./visdrone_data"

# =========================================================
# 1. ë°ì´í„° ì¤€ë¹„ & Dataset
# =========================================================
def prepare_data():
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
    pass 

VISDRONE_CLASSES = ['pedestrian', 'people', 'bicycle', 'car', 'van', 'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor']
id_map = {i + 1: i for i in range(len(VISDRONE_CLASSES))}
# [ìˆ˜ì •] ì €ì¥ëœ í”„ë¡œì„¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
try:
    processor = DetrImageProcessor.from_pretrained(LOAD_DIR)
except:
    processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")

class VisDroneDataset(Dataset):
    def __init__(self, img_dir, label_dir, processor):
        self.img_dir = img_dir
        self.label_dir = label_dir
        self.processor = processor
        self.target_size = (800, 800)
        self.resize = transforms.Resize(self.target_size)
        self.img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.jpg')])

    def __len__(self):
        return len(self.img_files)

    def __getitem__(self, idx):
        try:
            file_name = self.img_files[idx]
            img_id = file_name.replace('.jpg', '')
            image = Image.open(os.path.join(self.img_dir, file_name)).convert("RGB")
            
            w_orig, h_orig = image.size
            image = self.resize(image)
            w_new, h_new = self.target_size
            
            scale_w = w_new / w_orig
            scale_h = h_new / h_orig

        except:
            return self.__getitem__((idx + 1) % len(self))

        boxes, labels, areas = [], [], []
        label_path = os.path.join(self.label_dir, img_id + '.txt')
        
        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                for line in f:
                    data = list(map(float, line.strip().replace(',', ' ').split()))
                    if len(data) > 5 and int(data[5]) in id_map:
                        x, y, w, h = data[0], data[1], data[2], data[3]
                        x *= scale_w
                        y *= scale_h
                        w *= scale_w
                        h *= scale_h
                        
                        if w > 0 and h > 0:
                            boxes.append([x, y, w, h])
                            labels.append(id_map[int(data[5])])
                            areas.append(w * h)
        
        if not boxes:
            boxes, labels, areas = [[0.0, 0.0, 0.0, 0.0]], [0], [0.0]
            
        target = {
            "image_id": idx,
            "annotations": [{"bbox": b, "category_id": l, "area": a, "iscrowd": 0} for b, l, a in zip(boxes, labels, areas)]
        }
        encoding = self.processor(images=image, annotations=target, return_tensors="pt")
        return {"pixel_values": encoding["pixel_values"].squeeze(), "labels": encoding["labels"][0]}

def collate_fn(batch):
    return {
        "pixel_values": torch.stack([item["pixel_values"] for item in batch]),
        "labels": [item["labels"] for item in batch]
    }

def find_dirs(base_path, target_folder):
    for root, dirs, files in os.walk(base_path):
        if "images" in dirs and target_folder in root:
            img_path = os.path.join(root, "images")
            lbl_path = os.path.join(root, "annotations") if "annotations" in dirs else os.path.join(root, "labels")
            return img_path, lbl_path
    return None, None

def plot_loss_graph(train_log, val_log, save_path):
    plt.figure(figsize=(10, 6))
    plt.plot(train_log, label='Train Loss', color='blue')
    plt.plot(val_log, label='Val Loss', color='red')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training & Validation Loss Curve')
    plt.legend()
    plt.grid(True)
    plt.savefig(f"{save_path}/loss_graph.png")
    print(f"ğŸ“Š Loss ê·¸ë˜í”„ ì €ì¥ë¨: {save_path}/loss_graph.png")

def generate_heatmap(model, loader, device, save_path):
    print("ğŸ§© í˜¼ë™ í–‰ë ¬(Heatmap) ìƒì„± ì¤‘... (ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)")
    model.eval()
    y_pred = []
    
    with torch.no_grad():
        for batch in tqdm(loader, desc="Evaluating for Heatmap"):
            pixel_values = batch["pixel_values"].to(device)
            outputs = model(pixel_values=pixel_values)
            target_sizes = torch.tensor([[800, 800]] * len(pixel_values)).to(device)
            results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.5)

            for result in results:
                pred_classes = result['labels'].cpu().numpy()
                y_pred.extend(pred_classes)

    plt.figure(figsize=(12, 8))
    if len(y_pred) > 0:
        sns.countplot(x=y_pred)
        plt.xticks(ticks=range(len(VISDRONE_CLASSES)), labels=VISDRONE_CLASSES, rotation=45)
        plt.title("Predicted Object Distribution")
        plt.savefig(f"{save_path}/prediction_heatmap.png")
        print(f"ğŸ”¥ ì˜ˆì¸¡ ë¶„í¬ íˆíŠ¸ë§µ ì €ì¥ë¨: {save_path}/prediction_heatmap.png")
    else:
        print("âš ï¸ íƒì§€ëœ ê°ì²´ê°€ ì—†ì–´ì„œ íˆíŠ¸ë§µì„ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# =========================================================
# 3. ë©”ì¸ ì‹¤í–‰ë¶€
# =========================================================
if __name__ == '__main__':
    prepare_data()
    TRAIN_IMG, TRAIN_LBL = find_dirs(DATA_DIR, "train")
    VAL_IMG, VAL_LBL = find_dirs(DATA_DIR, "val")
    
    if not TRAIN_IMG: 
        TRAIN_IMG, TRAIN_LBL = find_dirs(DATA_DIR, "VisDrone2019-DET-train")
        VAL_IMG, VAL_LBL = find_dirs(DATA_DIR, "VisDrone2019-DET-val")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”¥ í•™ìŠµ ì‹œì‘ (GPU: {torch.cuda.get_device_name(0)})")
    print(f"ğŸ¯ ëª©í‘œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {TARGET_BATCH_SIZE} (ë¬¼ë¦¬ì  ë°°ì¹˜: {PHYSICAL_BATCH_SIZE} x ëˆ„ì : {ACCUMULATION_STEPS})")
    
    train_ds = VisDroneDataset(TRAIN_IMG, TRAIN_LBL, processor)
    val_ds = VisDroneDataset(VAL_IMG, VAL_LBL, processor)
    
    train_loader = DataLoader(train_ds, batch_size=PHYSICAL_BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)
    val_loader = DataLoader(val_ds, batch_size=PHYSICAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS)

    # [ìˆ˜ì •] ì´ì–´ì„œ í•™ìŠµí•˜ê¸° ìœ„í•´ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ“‚ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘... ({LOAD_DIR})")
    try:
        model = DetrForObjectDetection.from_pretrained(LOAD_DIR, ignore_mismatched_sizes=True).to(device)
    except:
        print("âš ï¸ ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
        model = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50", num_labels=10, ignore_mismatched_sizes=True).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    scaler = GradScaler() # [ì¶”ê°€] FP16 ìŠ¤ì¼€ì¼ëŸ¬

    train_loss_history = []
    val_loss_history = []
    
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)

    print(f"ğŸš€ ì¶”ê°€ í•™ìŠµ ì‹œì‘! (+{EPOCHS} Epoch)")
    
    for epoch in range(EPOCHS):
        # --- Train ---
        model.train()
        train_loss = 0
        optimizer.zero_grad() # ì‹œì‘ ì „ ì´ˆê¸°í™”
        
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]")
        
        for i, batch in enumerate(loop):
            pixel_values = batch["pixel_values"].to(device)
            labels = [{k: v.to(device) for k, v in t.items()} for t in batch["labels"]]
            
            # [ì¶”ê°€] Mixed Precision (FP16) ì ìš©
            with autocast():
                outputs = model(pixel_values=pixel_values, labels=labels)
                # Lossë¥¼ ëˆ„ì  íšŸìˆ˜ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤ (í‰ê· ì„ ë§ì¶”ê¸° ìœ„í•´)
                loss = outputs.loss / ACCUMULATION_STEPS
            
            # [ì¶”ê°€] Scalerë¡œ ì—­ì „íŒŒ
            scaler.scale(loss).backward()
            
            # [í•µì‹¬] ì •í•´ì§„ íšŸìˆ˜(4ë²ˆ)ë§ˆë‹¤ ì—…ë°ì´íŠ¸
            if (i + 1) % ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            
            # ê¸°ë¡ìš© LossëŠ” ë‹¤ì‹œ ê³±í•´ì„œ ë³µì›
            train_loss += loss.item() * ACCUMULATION_STEPS
            loop.set_postfix(loss=loss.item() * ACCUMULATION_STEPS)
        
        avg_train_loss = train_loss / len(train_loader)
        train_loss_history.append(avg_train_loss) 

        # --- Val ---
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Val]"):
                pixel_values = batch["pixel_values"].to(device)
                labels = [{k: v.to(device) for k, v in t.items()} for t in batch["labels"]]
                outputs = model(pixel_values=pixel_values, labels=labels)
                val_loss += outputs.loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_loss_history.append(avg_val_loss) 
        
        print(f"ğŸ“Š Epoch {epoch+1} ì™„ë£Œ | Train: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f}")

        # --- ë¬´ì¡°ê±´ ì €ì¥ ---
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘... ({SAVE_DIR})")
        model.save_pretrained(SAVE_DIR)
        processor.save_pretrained(SAVE_DIR)
    
    print("\nğŸ‰ ëª¨ë“  ì¶”ê°€ í•™ìŠµ ì¢…ë£Œ!")
    
    plot_loss_graph(train_loss_history, val_loss_history, SAVE_DIR)
    
    model = DetrForObjectDetection.from_pretrained(SAVE_DIR).to(device)
    generate_heatmap(model, val_loader, device, SAVE_DIR)
    
    print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {SAVE_DIR}")